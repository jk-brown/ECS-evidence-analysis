---
title: "Using ECS sample to run matilda"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

The goal of this script is to run Matilda with each of the ECS distributions we sampled prior. 

```{r}
library(matilda)
options(matilda.verbose = FALSE)
library(parallel)
```

# Using ECS samples to run Matilda

We use ECS values sampled from the estimated parametric distributions from S20 to propagate the varying levels of uncertainty associated with evidence configurations to probabilistic climate projections. This provides an opportunity to better understand how different ECS evidence configurations affect temperature trajectories from a simple carbon cycle climate model. 

We use the SSP2-4.5 emission scenario to run the model with the perturbed ECS samples.

```{r}
# read in ecs samples as a list
ecs_sample_list <- as.list(readRDS("data/ecs_samples_from_gamma_dist.RDS"))

# read in scenario input file (ini)
ini_245 <- system.file("input/hector_ssp245.ini", package = "hector")

```

The scenario input file is used to initiate a `core` environment for Hector. Hector is the simple carbon-cycle climate model that is the engine behind Matilda's probabilistic projection framework. More details about Hector and its usage, visit the [Hecotor GitHub page](https://jgcri.github.io/hector/).

```{r}
# initiate model core
core <- newcore(ini_245)

```

The result will be a new core object that can will be a required input to run the model. 

# Generate values for other model parameters

Matilda works by running Hector multiple times to build a perturbed parameter ensemble, thus applying parameter uncertainty to model outputs. We need to produce parameter values to accompany the ECS values we sampled in previous steps of the workflow.

Parameter sets are generated in Matilda using `generate_params`. We use this function to produce `n` initial parameter sets (`init_params`).  

```{r}
# set seed for reproducible result
set.seed(123)

# sample size (should match ECS sample)
n = 10000

# generate parameters
init_params <- generate_params(core = core, draws = n)

```

The result will be a new data frame object with 15,000 samples for 6 parameters.

*NOTE*: This data frame includes a column for `ECS`. These are samples drawn from the default prior distribution in Matilda, not the distributions selected for this analysis. 

We replace the default generated `ECS` values with the values we sampled from S20 distributions. This gives us a set of model parameters that are identical except for the `ECS` column, which isolates the impact of propagating `ECS` uncertainty through the model. 

```{r}
# create a list of parameter data frames based on ECS samples in ecs_sample_list
parameter_list <- lapply(ecs_sample_list, function(ECS) {
  
  # copy init_params
  params_no_ecs <- init_params
  
  # remove the ECS column from the parameter data frame
  params_no_ecs$ECS <- NULL
  
  # add sampled S20 ecs values
  cbind(ECS, params_no_ecs)
  
})

```

The result is a list of parameter sets named after the evidence configuration used to produce the ECS values. 

# Run the model 

We use each of the parameter sets in `parameter_list` to run the model. This produces a single Hector run for each of the 15,000 parameter sets per each ECS evidence configuration (15,000 x 5 = 75,000 total model runs).

Parallel computing on the local machine is used to make this process as efficient as possible. 

```{r}
# split the parameters into chunks for each 'worker'
parameter_chunks_by_scenario <- lapply(parameter_list, function(df) {
  
  split(df, 1:100)
  
})

# detect cores 
detectCores()

# initiate a cluster
cl <- makeCluster(detectCores() - 1)

# export required functions and objects to run the model
clusterExport(cl, c("parameter_chunks_by_scenario",
                    "ini_245",
                    "newcore",
                    "reset",
                    "iterate_model"))
# start time
start_time <- Sys.time()

# run the model with parLapply
model_result <- parLapply(cl, parameter_chunks_by_scenario, function(evidence_scenario) {
  
  # initialize a model core for each loop iteration
  core <- newcore(ini_245)
  
  # run the model for each parameter chunk
  result_list <- lapply(evidence_scenario, function(chunk) {
    
    iterate_model(core = core,
                  params = chunk,
                  save_years = 1800:2100,
                  save_vars = c("gmst", 
                                "CO2_concentration", 
                                "ocean_uptake"))
  })

  # ensure correct run_number added to each parameter chunk
  for (i in 2:length(result_list)) {

    # calculate the max value of the previous element in result_list
    max_run_number <- max(result_list[[i - 1]]$run_number)

    # add the max value of the previous element to the run_number of the current
    # element to get a run_number that is continuous from the previous element.
    result_list[[i]]$run_number <- result_list[[i]]$run_number + max_run_number
  }
  
  # bind parameter_chunks
  result <- do.call(rbind, result_list)
  
  return(result)
})

# Stop time
run_time <- Sys.time() - start_time
print(run_time)

# stop the cluster
stopCluster(cl)

# save the result
saveRDS(model_result, "data/raw_unweighted_model_results.RDS")
```
This can take ~8 hours to run 50,000 model iterations (10,000 runs x 5 scenarios).

The result is `model_result` a list of Matilda outputs, one for each `ECS` configuration. Each result in the list contains 10,000 Hector runs using the parameters from prior steps for the years and variables identified in `iterate_model`. 

# Weight model runs

After the model is run, we weight the models using inbuilt criterion (observed CO2 concentration and observed global mean surface temperature). It is possible to also add other criterion, for example we may create a criterion with data for ocean carbon uptake. 

```{r}
# weight models using observed co2 and temp - store in a list
model_init_weights <- lapply(model_result, function(df) {
  
  # produce weights based on co2
  weights_co2 = score_runs(df, 
                           criterion = criterion_co2_obs(),
                          score_function = score_bayesian)
  # omit any NAs
  weights_co2 = na.omit(weights_co2)
  
  # produce weights based on temp
  weights_temp = score_runs(df,
                            criterion = criterion_gmst_obs(),
                            score_function = score_bayesian)
  # omit any NAs
  weights_temp = na.omit(weights_temp)
  
  # store in a list 
  list = list(weights_co2, weights_temp)
  
  return(list)
  
})
```

The result of this initial weighting step is a list of ECS configurations, each with a list of two data frames where weights are stored. The first data frame contains weights for each non-NA model using observed CO2 while the second data frame contains weights for each non-NA model using observed temperature data. We use the initial weights to run `multi_criteria_weighting` which computes a weight taking into account both the model likelihood based on observed temperature CO2 and the model likelihood based on observed temperature. 

```{r}
# compute weights combining both co2 and temp likelihood
model_weights <- lapply(model_init_weights, function(weight_list) {
  
  # use mulit_criteria_weighting to produces a weights data frame for each scenario
  weights_df <- multi_criteria_weighting(weight_list)
  
})
```

The result is a new list called `model_weights` which contains, in each ECS configuration element, a multi-criteria weight (`mc_weight`) for each non-NA model run.

We merge model weights with the original result based on run number. This produces a list (based on ECS scenario) of the full model results and the assigned weights for each run.

```{r}
# combine weights with results data frame - use Map to fun function across elements of two lists
weighted_model_results <- Map(
  
  # write function that take the result list and model_weights list
  function(results, model_weights) {
  
  # merge results and model_weights by run_number
  weighted_results = merge(results, model_weights, by = "run_number")
  
  # omit NAs from the result - these are models that produced an error during analysis
  weighted_results = na.omit(weighted_results)
  
  return(weighted_results)
  
}, model_result, model_weights) # provide the lists to use in our defined Map function.

```

We combine the list of weighted results to produce a single, primary long-format data frame of all the ECS configuration scenarios we are interested in analyzing. 

```{r}
# overwrite weighted_model_results to rename the scenario column to match the name of the element
weighted_model_results <- lapply(names(weighted_model_results), function(df_name) {
  
  # get the data frame corresponding with the current element name
  df <- weighted_model_results[[df_name]]
  
  # replace scenario values to match the name of the scenario
  df$scenario <- df_name
  
  # return the modified data frame
  return(df)
}) 

# bind elements of the model_result list to produce single data frame
weighted_result_df <- do.call(rbind, weighted_model_results)

# save weighted data frame in data directory
saveRDS(weighted_result_df, "data/weighted_model_results.RDS")
```

